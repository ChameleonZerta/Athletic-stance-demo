{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2023/05/02\n",
    "# @Author  : D_Chameleon\n",
    "# @File    : DEMO_network.ipynb\n",
    "# @Software: PyCharm & Visual Studio Code\n",
    "#########################################################\n",
    "import scipy.io as scio\n",
    "import numpy as np\n",
    "from sklearn.model_selection import *\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Data/BufferFeatures.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32me:\\Anaconda\\envs\\PyTorch\\lib\\site-packages\\scipy\\io\\matlab\\mio.py:39\u001b[0m, in \u001b[0;36m_open_file\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mopen\u001b[39;49m(file_like, mode), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIOError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     41\u001b[0m     \u001b[39m# Probably \"not found\"\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Data/BufferFeatures.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m feature \u001b[39m=\u001b[39m scio\u001b[39m.\u001b[39;49mloadmat(\u001b[39m'\u001b[39;49m\u001b[39m./Data/BufferFeatures.mat\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m acc \u001b[39m=\u001b[39m scio\u001b[39m.\u001b[39mloadmat(\u001b[39m'\u001b[39m\u001b[39m./Data/BufferedAccelerations.mat\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m feat \u001b[39m=\u001b[39m feature[\u001b[39m'\u001b[39m\u001b[39mfeat\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\PyTorch\\lib\\site-packages\\scipy\\io\\matlab\\mio.py:224\u001b[0m, in \u001b[0;36mloadmat\u001b[1;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[39mLoad MATLAB file.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[39m    3.14159265+3.14159265j])\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    223\u001b[0m variable_names \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mvariable_names\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 224\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_context(file_name, appendmat) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    225\u001b[0m     MR, _ \u001b[39m=\u001b[39m mat_reader_factory(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    226\u001b[0m     matfile_dict \u001b[39m=\u001b[39m MR\u001b[39m.\u001b[39mget_variables(variable_names)\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\PyTorch\\lib\\contextlib.py:113\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[0;32m    112\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[0;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\PyTorch\\lib\\site-packages\\scipy\\io\\matlab\\mio.py:17\u001b[0m, in \u001b[0;36m_open_file_context\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m@contextmanager\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_context\u001b[39m(file_like, appendmat, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m---> 17\u001b[0m     f, opened \u001b[39m=\u001b[39m _open_file(file_like, appendmat, mode)\n\u001b[0;32m     18\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m         \u001b[39myield\u001b[39;00m f\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\PyTorch\\lib\\site-packages\\scipy\\io\\matlab\\mio.py:45\u001b[0m, in \u001b[0;36m_open_file\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[39mif\u001b[39;00m appendmat \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m file_like\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.mat\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     44\u001b[0m         file_like \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.mat\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> 45\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mopen\u001b[39;49m(file_like, mode), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\n\u001b[0;32m     48\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mReader needs file name or open file-like object\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     49\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Data/BufferFeatures.mat'"
     ]
    }
   ],
   "source": [
    "feature = scio.loadmat('./Data/BufferFeatures.mat')\n",
    "acc = scio.loadmat('./Data/BufferedAccelerations.mat')\n",
    "feat = feature['feat']\n",
    "actnames = acc['actnames']\n",
    "actid = acc['actid']\n",
    "atx = acc['atx']\n",
    "aty = acc['aty']\n",
    "atz = acc['atz']\n",
    "\n",
    "actid_label = actid.reshape(1, -1)[0]-1\n",
    "\n",
    "feature_size = np.shape(feat)[1]\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "scaler_minmax = MinMaxScaler(feature_range=(-1, 1))\n",
    "feat_minmax = scaler_minmax.fit_transform(feat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练集与测试集构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_train, feat_test, actid_train, actid_test = \\\n",
    "    train_test_split(feat_minmax, actid_label, test_size=0.5)\n",
    "feat_train = torch.from_numpy(feat_train).to(torch.float).to(device)\n",
    "feat_test = torch.from_numpy(feat_test).to(torch.float).to(device)\n",
    "actid_train = torch.from_numpy(actid_train).to(torch.int64).to(device)\n",
    "actid_test = torch.from_numpy(actid_test).to(torch.int64).to(device)\n",
    "\n",
    "batch_size = 30\n",
    "train_dataset = TensorDataset(feat_train, actid_train)\n",
    "test_dataset = TensorDataset(feat_test, actid_test)\n",
    "train_iter = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_iter = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens= feature_size, 6, 100\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(num_inputs, num_hiddens),\n",
    "    # nn.Sigmoid(),\n",
    "    # nn.Linear(num_hiddens, num_hiddens),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(num_hiddens, num_outputs)\n",
    "    # torch交叉熵的损失函数自带softmax运算，这里就没有再加一层激活函数\n",
    "    ).to(device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in net.parameters():\n",
    "    nn.init.normal_(param, mean=0, std=0.005)\n",
    "\n",
    "\n",
    "def acc(test_iter, net, device):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for x, y in test_iter:\n",
    "        acc_sum += (net(x.to(device)).argmax(dim=1) == y.to(device)).sum().item()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum / n\n",
    "\n",
    "\n",
    "K = acc(test_iter, net, device)\n",
    "print(K)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉熵损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(params=net.parameters(), lr=0.02, weight_decay=0.0001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练与可视化输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_allacc = []\n",
    "test_allacc = []\n",
    "loss_all = []\n",
    "num_epochs = 250\n",
    "for epoch in range(num_epochs):\n",
    "    train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "    for x, y in train_iter:\n",
    "        y_hat = net(x)\n",
    "        lo = loss(y_hat, y).sum()\n",
    "        optimizer.zero_grad()  # 优化器的梯度清零\n",
    "        lo.backward()  # 反向传播梯度计算\n",
    "        optimizer.step()  # 优化器迭代\n",
    "        train_l_sum += lo.item()\n",
    "        train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "        n += y.shape[0]\n",
    "    test_acc = acc(test_iter, net, device)\n",
    "    train_allacc.append(train_acc_sum / n)\n",
    "    test_allacc.append(test_acc)\n",
    "    loss_all.append(batch_size * train_l_sum / n)\n",
    "    print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "          % (epoch + 1, batch_size * train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "plt.plot(loss_all, label='loss')\n",
    "plt.plot(train_allacc, label='train')\n",
    "plt.plot(test_allacc, label='test')\n",
    "plt.legend(loc='best')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
